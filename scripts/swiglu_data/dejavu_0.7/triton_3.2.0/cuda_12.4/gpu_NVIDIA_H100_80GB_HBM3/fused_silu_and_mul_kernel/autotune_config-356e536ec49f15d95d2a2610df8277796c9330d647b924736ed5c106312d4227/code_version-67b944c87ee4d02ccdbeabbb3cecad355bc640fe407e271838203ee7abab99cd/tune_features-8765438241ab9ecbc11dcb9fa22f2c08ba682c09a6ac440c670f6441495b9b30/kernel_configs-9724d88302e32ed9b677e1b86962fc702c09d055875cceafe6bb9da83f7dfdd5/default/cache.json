{
    "signature": "JITFunction(triton_swiglu:fused_silu_and_mul_kernel)",
    "total_bench_time_s": 865.0588414669037,
    "total_configs": 540,
    "current_eval": {},
    "keys": [
        "D",
        "num_tokens",
        "n_elements"
    ],
    "cache": {
        "('16', '16', '512', 'torch.float16', 'torch.float16')": "BLOCK_SIZE: 16, num_warps: 1, num_ctas: 1, num_stages: 0, num_buffers_warp_spec: 0, num_consumer_groups: 0, reg_dec_producer: 0, reg_inc_consumer: 0, maxnreg: None",
        "('32', '16', '1024', 'torch.float16', 'torch.float16')": "BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, num_buffers_warp_spec: 0, num_consumer_groups: 0, reg_dec_producer: 0, reg_inc_consumer: 0, maxnreg: None",
        "('64', '16', '2048', 'torch.float16', 'torch.float16')": "BLOCK_SIZE: 256, num_warps: 4, num_ctas: 1, num_stages: 2, num_buffers_warp_spec: 0, num_consumer_groups: 0, reg_dec_producer: 0, reg_inc_consumer: 0, maxnreg: None"
    },
    "timings": {
        "('16', '16', '512', 'torch.float16', 'torch.float16')": [
            0.0012462785234674811
        ],
        "('32', '16', '1024', 'torch.float16', 'torch.float16')": [
            0.0015729148872196674
        ],
        "('64', '16', '2048', 'torch.float16', 'torch.float16')": [
            0.0015838476829230785
        ]
    },
    "timings_data": {
        "labels": [
            "ms"
        ],
        "rep_t_ms": 100,
        "warmup_t_ms": 25,
        "cuda_graphs": true
    }
}